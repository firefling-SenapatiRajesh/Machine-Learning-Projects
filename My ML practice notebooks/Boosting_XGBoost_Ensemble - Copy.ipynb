{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_CPXwyUJOhE"
   },
   "source": [
    "This Notebook is authored by Rocky Jagtiani for all his learner friends enrolled on govt and private platforms. Connect with me here : https://linkedin.com/in/rocky-jagtiani-3b390649/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_CmF2UzKewJ"
   },
   "source": [
    "Introduction\n",
    "--\n",
    "**XGBoost** is an implementation of gradient boosted decision trees designed for speed and performance.  XGBoost stands for <b><font color='darkblue'> extreme gradient boosting </font></b>, which is an implementation of gradient boosting with several additional features focused on performance and speed.\n",
    "\n",
    "We have made predictions with the **random forest method**, which achieves better performance than a single decision tree simply by averaging the predictions of many decision trees.\n",
    "\n",
    "We refer to the random forest method as an **\"ensemble method\"**. By definition, ensemble methods combine the predictions of several models (e.g., several trees, in the case of random forests).\n",
    "\n",
    "Now in this NB, we'll learn about **another ensemble method** called **gradient boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UjVY4O-MDsF"
   },
   "source": [
    "Steps in Gradient Boosting\n",
    "--\n",
    "Gradient boosting is a method that goes through cycles to iteratively add models into an ensemble.\n",
    "\n",
    "It begins by initializing the ensemble with a single model, whose predictions can be pretty naive. (Even if its predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.)\n",
    "\n",
    "Then, we start the cycle:\n",
    "\n",
    "**First**, we use the current ensemble to generate predictions for each observation in the dataset. To make a prediction, we add the predictions from all models in the ensemble.\n",
    "\n",
    "These predictions are used to calculate a loss function (like mean squared error, for instance).\n",
    "\n",
    "Then, we use the loss function to fit a new model that will be added to the ensemble. Specifically, we determine model parameters so that adding this new model to the ensemble will reduce the loss. (*note: The \"gradient\" in \"gradient boosting\" refers to the fact that we'll use gradient descent on the loss function to determine the parameters in this new model.*)\n",
    "\n",
    "**Finally**, we add the new model to ensemble, and ...\n",
    "... **repeat**!\n",
    "\n",
    "![XGBOOst_Image](https://drive.google.com/uc?id=11kQ-s_oc1G-xHY5qGXubSGeI6YFZq3Sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mahWcuSQ7Ah"
   },
   "source": [
    "In the coded examples, we'll work with the **XGBoost library**. (<font color='darkblue'> xgboost.XGBRegressor  and  XGBClassifier</font> )\n",
    "\n",
    "<font color='green'> Note : Scikit-learn has another version of gradient boosting, but XGBoost has some technical advantages. The two classes in Scikit-learn equally popular are <i>sklearn.ensemble.GradientBoostingClassifier</i> and <i>sklearn.ensemble.GradientBoostingRegressor</i></font>\n",
    "\n",
    "As the NB progresses, you would see that XGBRegressor class has many **tunable parameters** (*as compared to sklearn.ensemble classes*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6X9y842wVXjg"
   },
   "outputs": [],
   "source": [
    "# importing required Libraries\n",
    "import numpy as np\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# importing some pre-defn datasets\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes, load_wine\n",
    "\n",
    "# importing different metrics for performance measurement\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error\n",
    "\n",
    "# importing modules for applying CROSS Validation and GridSearch on the dataset to find best parameters \n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "\n",
    "# importing XGBoost Library. It contains XGBRegressor and XGBClassifier modules\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09JyelOOc2t4"
   },
   "outputs": [],
   "source": [
    "# defining function to print scores in a formated way\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: {0}\\nMean: {1:.3f}\\nStd: {2:.3f}\".format(scores, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "TBzoCqsTdGI1",
    "outputId": "95eb56e0-661d-4794-d457-8ba55cdee459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03807591  0.05068012  0.06169621  0.02187235 -0.0442235  -0.03482076\n",
      "  -0.04340085 -0.00259226  0.01990842 -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 -0.02632783 -0.00844872 -0.01916334\n",
      "   0.07441156 -0.03949338 -0.06832974 -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 -0.00567061 -0.04559945 -0.03419447\n",
      "  -0.03235593 -0.00259226  0.00286377 -0.02593034]\n",
      " [-0.08906294 -0.04464164 -0.01159501 -0.03665645  0.01219057  0.02499059\n",
      "  -0.03603757  0.03430886  0.02269202 -0.00936191]\n",
      " [ 0.00538306 -0.04464164 -0.03638469  0.02187235  0.00393485  0.01559614\n",
      "   0.00814208 -0.00259226 -0.03199144 -0.04664087]]\n",
      "[151.  75. 141. 206. 135.]\n",
      "-----------------------------\n",
      "RMSE :  58.80307813639512\n"
     ]
    }
   ],
   "source": [
    "# Code Example 1 : Use XGBRegressor (i.e a regression model) to predict diabetes \n",
    "\n",
    "\n",
    "# diabetes dataset description like Column names and types here https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset.  \n",
    "X = diabetes.data\n",
    "y = diabetes.target # is a numeric value measuring diabetes(i.e sugar_level) one year after baseline\n",
    "                    # normal range of sugar_level for diabetes in a adult male is 125-180 mg/dL\n",
    "\n",
    "# see sample i/p features and the o/p target\n",
    "\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=10) # choose any positive value for random_state\n",
    "# objective=\"reg:squarederror\" is equivalent to objective=\"reg:linear\" and it tells the Regressor Model to minimise mse\n",
    "# mse --> mean squared error\n",
    "\n",
    "# split data into training and test set, for both features and target\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# predicting on test data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tAAXQp2n91h"
   },
   "outputs": [],
   "source": [
    "# optional\n",
    "# uncomment below line to print the xgb_model. You would see a very large no. of parameters.\n",
    "# xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "yL6Gn9wsoXAP",
    "outputId": "6731088e-f3e0-4410-9140-6160493b7e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  2]\n",
      " [ 1 89]]\n",
      "0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "# Code Example 2 : Use XGBClassifier (i.e a classifier model) to predict whether the female patient \n",
    "            # who have had breast_cancer in the past or some major symptoms would it \"recur\" in her or not ?  \n",
    "# so its a Binary classification problem. i.e either the outcome could be Malignant (means infectious or dangerous) \n",
    "# or Benign (not infectious or not dangerous )  \n",
    "# about the dataset here https://scikit-learn.org/dev/datasets/index.html#breast-cancer-dataset\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# split data into training and test set, for both features and target\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "# objective=\"binary:logistic\" tell the Classifier to use logistic Regression for classification\n",
    "\n",
    "\n",
    "\n",
    "# predicting on test data\n",
    "\n",
    "\n",
    "\n",
    "# your accuracy could be between 93 to 99%, which is very good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "sdM4ofv62cvs",
    "outputId": "34308dd1-d5ee-4159-fdb5-72ecd29a48b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 1 19  1]\n",
      " [ 0  0  8]]\n",
      "0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Code Example 3 : Use XGBClassifier (i.e a classifier model) to classify in which class does Wine sample belongs\n",
    "# So its a Multiclass classification problem.\n",
    "# This Wine_Dataset as just 3 Classes with [59,71,48] samples per class. \n",
    "# description of this dataset here : https://scikit-learn.org/stable/datasets/index.html#wine-dataset\n",
    "wine = load_wine()\n",
    "\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# split data into training and test set, for both features and target\n",
    "\n",
    "\n",
    "\n",
    "# objective=\"multi:softprob\" tells the algorithm to calculate probabilities for every possible outcome \n",
    "# (in this case, a probability for each of the three wine classes) and accordingly put the Wine sample into that category\n",
    "# for which we get the highest probability. Now, you may ask this Genius Qn : \"What if we get same probability ?\" well in\n",
    "# such case the sample is put in the first available class. for e.g. if p=0.8 for class_1 and class_2 then the sample\n",
    "# is put into class_1\n",
    "\n",
    "xgb_model.fit(train_X, train_y)\n",
    "\n",
    "y_pred = xgb_model.predict(test_X)\n",
    "\n",
    "print(confusion_matrix(test_y, y_pred))\n",
    "print(accuracy_score(test_y, y_pred))  # your accuracy could be between 93 to 99%, which is very good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHRaO7yC7fgu"
   },
   "source": [
    "<font color='red'> Q> Would you get more accurate performance metric by testing the XGRegressor by using CROSS Validation ?\n",
    "</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "OR8QTOyy6zie",
    "outputId": "f2696696-c9e9-4e31-8515-2f9bc7795788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [55.30444573 55.59151472 63.44642064 57.82986083 58.71808276]\n",
      "Mean: 58.178\n",
      "Std: 2.937\n"
     ]
    }
   ],
   "source": [
    "# Code Example 4 : Use XGRegressor over the same diabetes dataset using KFold Cross validation\n",
    "# In case if you want to revise CROSS Validation , refer my NB here : https://colab.research.google.com/drive/19FP6eA0EK6SpbGcneuzMyHQfxVpBy-j4 \n",
    "\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):   \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    scores.append(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "display_scores(np.sqrt(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIxzSdEX8knt"
   },
   "source": [
    "<font color='green'> I am sure you would have got a more accurate performance metric this time via CROSS Validating our data. Remember due to CROSS Validation the problem of Overfitting is resolved. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "2MSduqPo9Ftf",
    "outputId": "da00b58b-b8fe-42ea-ddd9-43f2c92c92bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [56.04057166 56.14039793 60.3213523  59.67532995 60.7722925 ]\n",
      "Mean: 58.590\n",
      "Std: 2.071\n"
     ]
    }
   ],
   "source": [
    "# Redoing coding example 4 using cross_val_score() \n",
    "# Cross-validation using cross_val_score()\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "display_scores(np.sqrt(-scores)) # we are negating the scores becoz the scores returned by cross_val_score() are negative\n",
    "                                 # so -scores makes it positive\n",
    "                                 # scoring=\"neg_mean_squared_error\" parameter was also discuused in CROSS Validation NB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkQYbAW8-MVM"
   },
   "source": [
    "Parameter Tuning\n",
    "--\n",
    "XGBoost has a few parameters that can dramatically affect accuracy and training speed. The <u>most important parameters</u> you should understand are:\n",
    "\n",
    "**n_estimators** <br>\n",
    "n_estimators specifies how many times to go through the modeling cycle described above. It is equal to the number of models that we include in the ensemble.\n",
    "\n",
    "> Too low a value causes underfitting, which leads to inaccurate predictions on both training data and test data.\n",
    "\n",
    "> Too high a value causes overfitting, which causes accurate predictions on training data, but inaccurate predictions on test data (which is what we care about).\n",
    "\n",
    "**Typical values range from 100-1000**, though this depends a lot on the ***learning_rate*** parameter.\n",
    "<hr>\n",
    "\n",
    "**learning_rate** <br>\n",
    "Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in.\n",
    "\n",
    "This means each tree we add to the ensemble helps us less. So, we can ***set a higher value for n_estimators without overfitting.*** If we use <font color='darkblue'><b>early stopping</b>, the appropriate number of trees will be determined automatically. </font> ( <small>Extra Reading : do Refer : https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/ </small>.   <small><b>Remember :</b> It is generally a good idea to select the early_stopping_rounds as a reasonable function of the total number of training epochs (10% in this case).</small> )\n",
    "\n",
    "<font color='darkgreen'><b>\n",
    "In general, a small learning rate and large number of estimators will yield more accurate XGBoost model, though it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets learning_rate=0.1.</b></font>\n",
    "\n",
    "<hr>\n",
    "\n",
    "**early_stopping_rounds**<br>\n",
    "`early_stopping_rounds` offers a way to automatically find the ideal value for n_estimators. Early stopping causes the model to stop iterating when the validation score stops improving, even if we are much below the value for `n_estimators`. It's smart to set a high value for n_estimators and then use early_stopping_rounds to find the optimal time to stop iterating.\n",
    "\n",
    "Since random chance sometimes causes a single round where validation scores don't improve, you need to specify a number for how many rounds of straight deterioration to allow before stopping. Setting early_stopping_rounds=5 or 10 is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores.\n",
    "\n",
    "When using early_stopping_rounds, you also need to set aside some data for calculating the validation scores - this is done by setting the `eval_set` parameter.  **for example**\n",
    "\n",
    "<font color='blue'><i>\n",
    "my_model = XGBRegressor(n_estimators=500) <br>\n",
    "my_model.fit(X_train, y_train, <br>\n",
    "             early_stopping_rounds=5, <br> \n",
    "             eval_set=[(X_valid, y_valid)], <br>\n",
    "             verbose=False)\n",
    "</i></font>\n",
    "\n",
    "<hr>\n",
    "\n",
    "**n_jobs**\n",
    "\n",
    "On larger datasets where runtime is a consideration, you can use `parallelism` to build your models faster. It's common to set the parameter `n_jobs` equal to the `number of cores` on your machine. On smaller datasets, this won't help.\n",
    "\n",
    "The resulting model won't be any better.  But, it's useful in large datasets where you would otherwise `spend a long time` waiting during the fit command.\n",
    "\n",
    "Here's the modified example: <br>\n",
    "<font color='blue'><i>\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4) <br>\n",
    "my_model.fit(X_train, y_train,  <br>\n",
    "             early_stopping_rounds=5,  <br>\n",
    "             eval_set=[(X_valid, y_valid)], <br>\n",
    "             verbose=False)\n",
    "             </i></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "id": "Fg8nFnwvInjK",
    "outputId": "6b2a33d9-e858-48e4-d429-c0ff8c97ab66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.076923\n",
      "Will train until validation_0-error hasn't improved in 5 rounds.\n",
      "[1]\tvalidation_0-error:0.076923\n",
      "[2]\tvalidation_0-error:0.076923\n",
      "[3]\tvalidation_0-error:0.06993\n",
      "[4]\tvalidation_0-error:0.06993\n",
      "[5]\tvalidation_0-error:0.055944\n",
      "[6]\tvalidation_0-error:0.062937\n",
      "[7]\tvalidation_0-error:0.055944\n",
      "[8]\tvalidation_0-error:0.048951\n",
      "[9]\tvalidation_0-error:0.041958\n",
      "[10]\tvalidation_0-error:0.041958\n",
      "[11]\tvalidation_0-error:0.034965\n",
      "[12]\tvalidation_0-error:0.034965\n",
      "[13]\tvalidation_0-error:0.041958\n",
      "[14]\tvalidation_0-error:0.034965\n",
      "[15]\tvalidation_0-error:0.041958\n",
      "[16]\tvalidation_0-error:0.041958\n",
      "Stopping. Best iteration:\n",
      "[11]\tvalidation_0-error:0.034965\n",
      "\n",
      "-------------- accuracy_score is ----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code Example 5 : Use XGBClassifier with Parameters. This could save time for training and give better performance too. \n",
    "            \n",
    "# n_estimators => the number of boosted trees to train \n",
    "# early_stopping_rounds => training continues until validation has not improved in n rounds. \n",
    "# eval_set=[(X_test, y_test)] => indicates what parameters to evaluate\n",
    "# eval_metric=\"error\"  => indicates on what metric to take decision\n",
    "#              error => Binary classification error rate. It is calculated as #(wrong cases)/#(all cases)\n",
    "\n",
    "# so if we keep n_estimators = 150 (default is 100) and early_stopping_rounds = 5 then \n",
    "# if for 5 runs if our metric say accuracy_score stops improving then the XGBoost model stops running.\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# if more than one evaluation metric are given the last one is used for early stopping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNm6b_D5edVE"
   },
   "source": [
    "<font color='darkblue'><b> Important </b> </font> <br>\n",
    "**xgb_model.fit()** will produce a model from the last iteration, ***not the best one***, so to get the optimum model consider `retraining` over **xgb_model.best_iteration** rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "PsSXwU-7e6p-",
    "outputId": "07642bd7-2361-4a18-a728-d4b8c6325662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.034965, best iteration: 11, best ntree limit 12\n"
     ]
    }
   ],
   "source": [
    "print(\"best score: {0}, best iteration: {1}, best ntree limit {2}\".format(xgb_model.best_score, xgb_model.best_iteration, xgb_model.best_ntree_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "i1tldszAjgaY",
    "outputId": "985d8bc5-b07d-4307-a7cc-39ee8aa6b4e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.076923\n",
      "[1]\tvalidation_0-error:0.076923\n",
      "[2]\tvalidation_0-error:0.076923\n",
      "[3]\tvalidation_0-error:0.06993\n",
      "[4]\tvalidation_0-error:0.06993\n",
      "[5]\tvalidation_0-error:0.055944\n",
      "[6]\tvalidation_0-error:0.062937\n",
      "[7]\tvalidation_0-error:0.055944\n",
      "[8]\tvalidation_0-error:0.048951\n",
      "[9]\tvalidation_0-error:0.041958\n",
      "[10]\tvalidation_0-error:0.041958\n",
      "[11]\tvalidation_0-error:0.034965\n",
      "-------------- accuracy_score is ----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best ntree limit 12 => means you should fix the n_estimators parameter to 12 in XGBClassifier\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=10, eval_metric=\"error\", n_estimators=12) \n",
    "\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"-------------- accuracy_score is ----------------\")\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-y0H1wtnM2f"
   },
   "source": [
    "Wow !!\n",
    "--\n",
    "\n",
    "You have done a lot by now. Let me recall :\n",
    "<font color = 'green'> <br>\n",
    "1. You understood the steps in Gradient Boosting. i.e add a new model to ensemble in each round till we get good performance metric.\n",
    "\n",
    "2. Then you used XGBoost library i.e used two classes of it <b>XGBRegressor</b> and <b>XGBClassifier</b>.\n",
    "\n",
    "3. You run through some codes where : <br>\n",
    "<font color = 'green'>Example 1 :</font> Used XGBRegressor (i.e a regression model) to predict diabetes.\n",
    "<br><font color = 'green'>Example 2 :</font> Used XGBClassifier (i.e a classifier model) to predict the cancer is dangerous or not ? This was a <b>Binary Classifier Problem</b>\n",
    "<br><font color = 'green'>Example 3 :</font> Again we used XGBClassifier over the Wine_dataset to classify it into one of the three classes. This was a <b>Multi-Classification Problem</b>\n",
    "<br><font color = 'green'>Example 4 :</font> You smartly applied CROSS VALIDATION <br>\n",
    "<b> You learned the 4 most used parameters, which when <u>tuned</u> improve the performance of XGBoost </b>\n",
    "<br><font color = 'green'>Example 5 :</font>  You smartly applied the parameters and <b><u>tuned</u></b> the Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OLfQcfLrPLH"
   },
   "source": [
    "Resources & References\n",
    "--\n",
    "1. <a href='https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e'>Good Blog article_1</a>\n",
    "\n",
    "2. <a href='https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/'>Good Blog article_2</a>\n",
    "\n",
    "3. <a href='https://xgboost.readthedocs.io/en/latest/parameter.html'>Official Docs of XGBoost parameters</a>\n",
    "\n",
    "4. <a href='https://xgboost.readthedocs.io/en/latest/tutorials/index.html'>Official Docs on XGBoost tutorials and Examples</a>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Boosting_XGBoost_Ensemble.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
